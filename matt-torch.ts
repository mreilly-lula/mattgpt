import { log } from "node:console";
import { Tensor, Value } from "./structures";

// super basic random sample algorithm, I'm sure its good enough
export function weightedRandomSample(
  probabilities: number[],
  numSamples: number
): number[] {
  const allowedResultValues = Array.from(
    { length: probabilities.length },
    (_, index) => index
  );

  let result: number[] = [];
  for (let step = 0; step < numSamples; step++) {
    const randomNumber = randFloat(0, 1);
    let cursor = 0;
    for (let i = 0; i < probabilities.length; i++) {
      cursor += probabilities[i];
      if (cursor >= randomNumber) {
        result.push(allowedResultValues[i]);
        break;
      }
    }
  }
  return result;
}

export function shuffle(array: any[]): any[] {
  let result = array;
  for (let i = result.length - 1; i > 0; i--) {
    const j = Math.floor(Math.random() * (i + 1));
    [result[i], result[j]] = [result[j], result[i]];
  }
  return result;
}

/**
 * Samples indices from a categorical distribution defined by `values`.
 * - `values` must be a 1-D tensor containing non-negative weights (not necessarily normalized).
 * - Returns `numSamples` draws with replacement.
 */
export function multinomial(values: Tensor, numSamples: number): number[] {
  if (values.dims.length !== 1) {
    throw new Error("multinomial currently only supports 1-D tensors");
  }
  if (numSamples <= 0) {
    return [];
  }

  const weights = values.vrow([]);
  const totalWeight = weights.reduce((acc, cur) => acc + cur.data, 0);
  if (totalWeight <= 0) {
    throw new Error("Sum of weights must be positive to sample");
  }

  const probabilities = weights.map((w) => w.data / totalWeight);
  const draws: number[] = [];

  for (let sample = 0; sample < numSamples; sample++) {
    const r = randFloat(0, 1);
    let cursor = 0;
    for (let i = 0; i < probabilities.length; i++) {
      cursor += probabilities[i];
      if (r <= cursor) {
        draws.push(i);
        break;
      }
    }
  }

  return draws;
}

export const randInt = (low: number, high: number): number =>
  Math.floor(
    Math.random() * (Math.floor(high) - Math.ceil(low) + 1) + Math.ceil(low)
  );
export const randFloat = (low: number, high: number): number =>
  Math.random() * (high - low) + low;
export const sum = (arr: number[], start?: number): number =>
  arr.reduce((acc, cur) => (acc += cur), start ?? 0);
export const zip = (a: any[], b: any[]): any[][] => {
  let result = [];
  const maxCompatLength = Math.min(a.length, b.length);
  for (let i = 0; i < maxCompatLength; i++) {
    result.push([a[i], b[i]]);
  }

  return result;
};

type DeepCastToValue<T> = T extends number
  ? Value
  : T extends Array<infer U>
  ? DeepCastToValue<U>[]
  : never;

export function valuize<T>(data: T): DeepCastToValue<T> {
  if (Array.isArray(data)) {
    return data.map((item) => valuize(item)) as DeepCastToValue<T>;
  }

  return new Value(data as number) as DeepCastToValue<T>;
}

export const loss = (predictions: Value[], targets: Value[]) => {
  const combined = zip(targets, predictions);
  let result: Value[] = [];
  for (const combo of combined) {
    // calculate distance
    result.push(combo[0].subtract(combo[1]).pow(2));
  }
  return result.reduce((acc: Value, cur: Value) => {
    return acc.add(cur);
  }, new Value(0));
};

export const oneHot = (vals: number[], num_classes: number): number[][] => {
  let result: number[][] = [];
  for (let i = 0; i < vals.length; i++) {
    const zerosRow: number[] = new Array(num_classes).fill(0.0);
    zerosRow[vals[i]] = 1.0;
    result.push(zerosRow);
  }

  return result;
};

// box muller transform to get random numbers over a normal gaussian distribution
// https://en.wikipedia.org/wiki/Box%E2%80%93Muller_transform
export const randomNormal = (): number => {
  const u = 1 - Math.random();
  const v = Math.random();
  return Math.sqrt(-2.0 * Math.log(u)) * Math.cos(2.0 * Math.PI * v);
};

export const multiply = (a: Tensor, b: Tensor): Tensor => {
  if (a.dims.length !== 2 || b.dims.length !== 2) {
    throw new Error("I can only do 2d matrices right now");
  }

  if (a.dims[1] !== b.dims[0]) {
    throw new Error("Cant multiply these");
  }

  const rows = a.dims[0];
  const shared = a.dims[1];
  const cols = b.dims[1];
  const result: Tensor = new Tensor([rows, cols], 0);
  result.shape()
  // Cache rows/columns so the inner loop only performs Value ops.
  const aRows: Value[][] = new Array(rows);
  for (let i = 0; i < rows; i++) {
    aRows[i] = a.vrow([i]);
  }

  const bCols: Value[][] = new Array(cols);
  for (let j = 0; j < cols; j++) {
    const column = new Array(shared);
    for (let k = 0; k < shared; k++) {
      column[k] = b.at([k, j]);
    }
    bCols[j] = column as Value[];
  }

  for (let i = 0; i < rows; i++) {
    const row = aRows[i];
    for (let j = 0; j < cols; j++) {
      const col = bCols[j];
      let sum: Value | null = null;
      for (let k = 0; k < shared; k++) {
        const product = row[k].multiply(col[k]);
        sum = sum ? sum.add(product) : product;
      }
      result.set([i, j], sum ?? new Value(0));
    }
  }

  return result;
};

export const softmax = (t: Tensor): Tensor => {
  const exponentiated = t.map((item) => item.exp());
  return exponentiated.normalize();
};

export const arrange = (count: number): Tensor => {
  const filler = new Array(count).fill(0).map((_, index) => index);
  return Tensor.fromNestedArray([count], filler);
};

export const crossEntropy = (logits: Tensor, target: Tensor): Value => {
  const sMax = softmax(logits);
  const rowIdxs = arrange(sMax.dims[0]);
  const picked = rowIdxs.map((_, [row]) => {
    const col = target.at([row]).data;
    return sMax.at([row, col]);
  });
  const negLogLikelihood = picked
    .map((val) => val.log())
    .sum()
    .divide(new Value(picked.size))
    .negative();
  return negLogLikelihood;
};
